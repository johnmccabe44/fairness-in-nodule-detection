{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SUMMIT datasplit\n",
    "\n",
    "Purpose: The MONAI detection algorithm is based on nnDetection which takes very specific inputs i.e., nifti files. The algorithm uses LUNA16 as the baseline dataset and as such conforms to the competition rules i.e., 10 fold validation. The files need to be prepped in a very particular fashion in order to work out-of-box with the detection algorithm.\n",
    "\n",
    "SUMMIT is a much larger dataset so can probably support a single training set of 5K samples and 1K validation (plus hold out for later evaluation), therefore it is necessary to convert the SUMMIT metadata, which is in the form of a flat csv file containing the list of nodules to the format required by MONAI detection which is in the form a json file.\n",
    "\n",
    "This json file has the form:\n",
    "\n",
    "    {\n",
    "        \"training\" : [\n",
    "        {\n",
    "            \"box\": [\n",
    "                [\n",
    "                    real_world_x,\n",
    "                    real_world_y,\n",
    "                    real_world_z,\n",
    "                    xd,\n",
    "                    yd,\n",
    "                    zd\n",
    "                ],\n",
    "                [\n",
    "                    ...\n",
    "                ]\n",
    "            ],\n",
    "            \"image\": \"location of mhd on disc ... relative to a base root directory of the images\",\n",
    "            \"label\": [\n",
    "                0,\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        ...\n",
    "        ],\n",
    "        \"validation\" : [\n",
    "            {...},\n",
    "            ...\n",
    "            {...}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "This workbook transforms the training_metadata.csv and validation_metadata.csv into this form. due to the size and exploratory nature of this part of the project, only one fold will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5124 25\n"
     ]
    }
   ],
   "source": [
    "METADATA_PATH = '/Users/john/Projects/SOTAEvaluationNoduleDetection/output/metadata'\n",
    "\n",
    "data_splits = ['training','validation']\n",
    "\n",
    "\n",
    "summit_subset = {'training' : [], 'validation' : []}\n",
    "summit_datasplits = {'training' : [], 'validation' : []}\n",
    "\n",
    "i = j = 0\n",
    "\n",
    "for data_split in data_splits:\n",
    "\n",
    "    scans = pd.read_csv(Path(METADATA_PATH, data_split + '_scans.csv'))\n",
    "    metadata = pd.read_csv(Path(METADATA_PATH, data_split + '_metadata.csv'))\n",
    "    \n",
    "    \n",
    "    for scan_id in scans.scan_id.tolist():\n",
    "        study_id = scan_id.split('_',1)[0]\n",
    "\n",
    "        scan_item = {\n",
    "            'box' : [], \n",
    "            'image' : f'{study_id}/{scan_id}.mhd',\n",
    "            'label' : []\n",
    "        }\n",
    "\n",
    "        for idx, row in metadata[metadata.main_participant_id==study_id].iterrows():\n",
    "            scan_item['box'].append(\n",
    "                [\n",
    "                    row.nodule_x_coordinate,\n",
    "                    row.nodule_y_coordinate,\n",
    "                    row.nodule_z_coordinate,\n",
    "                    row.nodule_diameter_mm,\n",
    "                    row.nodule_diameter_mm,\n",
    "                    row.nodule_diameter_mm\n",
    "                ])\n",
    "            scan_item['label'].append(0)\n",
    "\n",
    "        summit_datasplits[data_split].append(scan_item)\n",
    "        i += 1\n",
    "\n",
    "        if Path('/Users/john/Projects/SOTAEvaluationNoduleDetection/scans/lung50',scan_item['image']).exists():\n",
    "            summit_subset[data_split].append(scan_item)\n",
    "            j+= 1\n",
    "\n",
    "\n",
    "print(i, j)\n",
    "\n",
    "Path('SUMMIT_datasplit/mhd_original').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open('SUMMIT_datasplit/mhd_original/dataset_fold0.json','w') as f:\n",
    "    json.dump(summit_datasplits, f)\n",
    "\n",
    "with open('SUMMIT_datasplit/mhd_original/dataset_fold0_subset.json','w') as f:\n",
    "    json.dump(summit_subset, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
