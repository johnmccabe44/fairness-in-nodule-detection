{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Combine Tranche1 and Tranche2 data into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = ['UCLH_43663037','UCLH_45634500','UCLH_59066126','UCLH_50882667','UCLH_27847999'] + ['UCLH_92436946']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranche1_scans = pd.read_csv('/Users/john/Projects/SOTAEvaluationNoduleDetection/metadata/lsut/tranche1_scans.csv')\n",
    "tranche2_scans = pd.read_csv('/Users/john/Projects/SOTAEvaluationNoduleDetection/metadata/lsut/tranche2_scans.csv')\n",
    "\n",
    "lsut_scans = pd.concat([tranche1_scans, tranche2_scans], ignore_index=True)\n",
    "\n",
    "lsut_scans = lsut_scans[~lsut_scans['scan_id'].isin(blacklist)]\n",
    "lsut_scans.to_csv('/Users/john/Projects/SOTAEvaluationNoduleDetection/metadata/lsut/lsut_scans.csv', index=False)\n",
    "\n",
    "lsut_scans_ids = set(lsut_scans['scan_id'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scans metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranche1_scans_metadata = pd.read_csv('/Users/john/Projects/SOTAEvaluationNoduleDetection/metadata/lsut/tranche1_scan_metadata.csv')\n",
    "tranche2_scans_metadata = pd.read_csv('/Users/john/Projects/SOTAEvaluationNoduleDetection/metadata/lsut/tranche2_scan_metadata.csv')\n",
    "\n",
    "lsut_scans_metadata = pd.concat([tranche1_scans_metadata, tranche2_scans_metadata], ignore_index=True)\n",
    "lsut_scans_metadata = lsut_scans_metadata[~lsut_scans_metadata['ScananonID'].isin(blacklist)]\n",
    "lsut_scans_metadata.to_csv('/Users/john/Projects/SOTAEvaluationNoduleDetection/metadata/lsut/lsut_scans_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodule Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tranche1_nodules = pd.read_csv('/Users/john/Projects/SOTAEvaluationNoduleDetection/metadata/lsut/tranche1_metadata.csv')\n",
    "tranche2_nodules = pd.read_csv('/Users/john/Projects/SOTAEvaluationNoduleDetection/metadata/lsut/tranche2_metadata.csv')\n",
    "\n",
    "lsut_metadata = pd.concat([tranche1_nodules, tranche2_nodules], ignore_index=True)\n",
    "lsut_metadata = lsut_metadata[~lsut_metadata['scan_id'].isin(blacklist)]\n",
    "\n",
    "def nms(df, scan_id_col, x_col, y_col, z_col, diameter_col, iou_threshold=0.5):\n",
    "\n",
    "    def iou(box1, box2):\n",
    "        x1, y1, z1, d1 = box1\n",
    "        x2, y2, z2, d2 = box2\n",
    "\n",
    "        r1 = d1 / 2\n",
    "        r2 = d2 / 2\n",
    "\n",
    "        dist = np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2 + (z1 - z2) ** 2)\n",
    "        return min(r1, r2) / max(r1, r2) if dist < r1 + r2 else 0\n",
    "\n",
    "    def nms_single_scan(scan_df):\n",
    "        boxes = scan_df[[x_col, y_col, z_col, diameter_col]].values\n",
    "        scores = np.ones(len(boxes))  # Assuming all nodules have the same score\n",
    "\n",
    "        indices = np.argsort(scores)[::-1]\n",
    "        keep = []\n",
    "\n",
    "        while len(indices) > 0:\n",
    "            current = indices[0]\n",
    "            keep.append(current)\n",
    "            if len(indices) == 1:\n",
    "                break\n",
    "\n",
    "            current_box = boxes[current]\n",
    "            rest_boxes = boxes[indices[1:]]\n",
    "\n",
    "            ious = np.array([iou(current_box, box) for box in rest_boxes])\n",
    "            indices = indices[1:][ious < iou_threshold]\n",
    "\n",
    "        return scan_df.iloc[keep]\n",
    "\n",
    "    result_df = df.groupby(scan_id_col).apply(nms_single_scan).reset_index(drop=True)\n",
    "    return result_df\n",
    "\n",
    "lsut_metadata = nms(lsut_metadata, 'scan_id', 'nodule_x_coordinate', 'nodule_y_coordinate', 'nodule_z_coordinate', 'nodule_diameter_mm')\n",
    "lsut_metadata.to_csv('/Users/john/Projects/SOTAEvaluationNoduleDetection/metadata/lsut/lsut_metadata.csv', index=False)\n",
    "\n",
    "\n",
    "lsut_metadata['scan_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check we have everything processed for GRT123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 297 set()\n"
     ]
    }
   ],
   "source": [
    "grt123_listings = (\n",
    "    pd.read_csv('grt123_cache_listings.txt', header=None, names=['filename'])\n",
    "    .drop(index=0)\n",
    "    .assign(filename=lambda df: df['filename'].str.replace('.npy', '', regex=False))\n",
    "    .assign(scan_id=lambda df: df['filename'].apply(lambda x: '_'.join(x.split('_')[:-1])),\n",
    "        filetype=lambda df: df['filename'].apply(lambda x: x.split('_')[-1]))\n",
    ")\n",
    "grt123_listings_ids = set(grt123_listings.query('filetype == \"clean\"')['scan_id'].values)\n",
    "\n",
    "print(len(lsut_scans_ids), len(grt123_listings_ids), lsut_scans_ids - grt123_listings_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that we have preprocessed everything for TicNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 297 set()\n"
     ]
    }
   ],
   "source": [
    "ticnet_listings = (\n",
    "    pd.read_csv('ticnet_cache_listings.txt', header=None, names=['filename'])\n",
    "    .drop(index=0)\n",
    "    .assign(filename=lambda df: df['filename'].str.replace('.npy', '', regex=False))\n",
    "    .assign(filetype=lambda df: df['filename'].apply(lambda x: x.split('_')[-1] if len(x.split('_')) == 3 else 'image'))\n",
    ")\n",
    "ticnet_preprocessed_ids = set(ticnet_listings.query('filetype == \"image\"')['filename'].values)\n",
    "\n",
    "print(len(lsut_scans_ids), len(ticnet_preprocessed_ids), lsut_scans_ids - ticnet_preprocessed_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that we have preprocessed everything for Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 297 set()\n"
     ]
    }
   ],
   "source": [
    "detection_listings = (\n",
    "    pd.read_csv('detection_cache_listings.txt', header=None, names=['foldername'])\n",
    "    .drop(index=0)\n",
    ")\n",
    "\n",
    "detection_preprocessed_ids = set(detection_listings['foldername'].values)\n",
    "\n",
    "print(len(lsut_scans_ids), len(detection_preprocessed_ids), lsut_scans_ids - detection_preprocessed_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the dataset json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json = {'trainings': [], 'validations': [], 'test': []}\n",
    "\n",
    "for scan_id in lsut_scans_ids:\n",
    "\n",
    "    group = lsut_metadata.query('scan_id == @scan_id')\n",
    "\n",
    "    if group.shape[0] == 0:\n",
    "\n",
    "        dataset_json['test'].append({\n",
    "            'image' : f'{scan_id}/{scan_id}.nii.gz',\n",
    "            'box' : [],\n",
    "            'label' : []\n",
    "        })\n",
    "\n",
    "    else:\n",
    "\n",
    "        dataset_json['test'].append({\n",
    "            'image' : f'{scan_id}/{scan_id}.nii.gz',\n",
    "            'box' : group[[\n",
    "                'nodule_x_coordinate',\n",
    "                'nodule_y_coordinate',\n",
    "                'nodule_z_coordinate',\n",
    "                'nodule_diameter_mm'\n",
    "            ]].values.tolist(),\n",
    "            'label' : [0] * group.shape[0]\n",
    "        })\n",
    "\n",
    "import json\n",
    "with open('/Users/john/Projects/SOTAEvaluationNoduleDetection/models/detection/datasplits/lsut/dataset.json', 'w') as f:\n",
    "    json.dump(dataset_json, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
