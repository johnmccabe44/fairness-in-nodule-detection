{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double check sample numbers are correct for training SUMMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/john/Projects/SOTAEvaluationNoduleDetection\n",
      "Training scans: 4754\n",
      "Training metadata: 6073\n",
      "Training unique metadata scans: 2657\n",
      "Training excludes: 1473\n",
      "Validation scans: 392\n",
      "Validation metadata: 468\n",
      "Validation unique metadata scans: 210\n",
      "Validation excludes: 112\n",
      "Test scans: 797\n",
      "Test metadata: 1082\n",
      "Test unique metadata scans: 438\n",
      "Test excludes: 210\n",
      "Total scans in all datasets: 5943\n",
      "Total metadata in all datasets: 7623\n",
      "Unique scans in metadata: 3305\n",
      "Total excludes in all datasets: 1795\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "workspace_path = Path(os.getcwd()).parent.parent\n",
    "\n",
    "print(workspace_path)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'Training' : {},\n",
    "    'Validation': {},\n",
    "    'Test' : {}\n",
    "}\n",
    "\n",
    "for ds in datasets:\n",
    "\n",
    "    datasets[ds]['scans'] = pd.read_csv(f'{workspace_path}/metadata/summit/partial/{ds}_scans.csv')\n",
    "    datasets[ds]['metadata'] = pd.read_csv(f'{workspace_path}/metadata/summit/partial/{ds}_metadata.csv').assign(scan_id=lambda df: df.participant_id + '_Y0_BASELINE_A')\n",
    "    datasets[ds]['excludes'] = pd.read_csv(f'{workspace_path}/metadata/summit/partial/{ds}_excludes.csv').assign(scan_id=lambda df: df.participant_id + '_Y0_BASELINE_A')\n",
    "\n",
    "\n",
    "    print(f'{ds} scans: {datasets[ds][\"scans\"].shape[0]}')\n",
    "    print(f'{ds} metadata: {datasets[ds][\"metadata\"].shape[0]}')\n",
    "    print(f'{ds} unique metadata scans: {datasets[ds][\"metadata\"][\"scan_id\"].nunique()}')\n",
    "    print(f'{ds} excludes: {datasets[ds][\"excludes\"].shape[0]}')\n",
    "\n",
    "print('Total scans in all datasets:', sum([datasets[ds]['scans'].shape[0] for ds in datasets]))\n",
    "print('Total metadata in all datasets:', sum([datasets[ds]['metadata'].shape[0] for ds in datasets]))\n",
    "print('Unique scans in metadata:', sum([datasets[ds]['metadata']['scan_id'].nunique() for ds in datasets]))\n",
    "print('Total excludes in all datasets:', sum([datasets[ds]['excludes'].shape[0] for ds in datasets]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans fields: Index(['scan_id'], dtype='object')\n",
      "Metadata fields: Index(['form_instance_id', 'form_instance_status', 'participant_id',\n",
      "       'form_instance_index', 'nodule_brock_score', 'nodule_category',\n",
      "       'nodule_diameter_mm', 'nodule_lesion_id', 'nodule_lung_rads',\n",
      "       'nodule_mass', 'nodule_mass_core', 'nodule_mass_double_time_core',\n",
      "       'nodule_mass_doubling_time', 'nodule_reliable_segment', 'nodule_site',\n",
      "       'nodule_size_volume_cub_mm', 'nodule_slice_number',\n",
      "       'nodule_spiculation', 'nodule_subsolid_major_axis_diameter',\n",
      "       'nodule_type', 'nodule_volume_core', 'nodule_volume_doubling_time',\n",
      "       'nodule_volume_percentage_change',\n",
      "       'nodule_volume_volume_double_time_core', 'nodule_x_coordinate',\n",
      "       'nodule_y_coordinate', 'nodule_z_coordinate',\n",
      "       'radiology_report_management_plan_value', 'management_plan',\n",
      "       'radiology_report_malignancy_diagnosis',\n",
      "       'radiology_report_malignancy_criteria',\n",
      "       'radiology_report_malignancy_primary_order',\n",
      "       'radiology_report_scans_transfer_state', 'gender', 'age_group',\n",
      "       'ethnic_group', 'radiology_report_lesions_to_exclude', 'scan_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# What fields are in the datasets\n",
    "print('Scans fields:', datasets['Training']['scans'].columns)\n",
    "print('Metadata fields:', datasets['Training']['metadata'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset.json for MONAI Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 2657\n",
      "Training boxes: 6073\n",
      "Validation: 392\n",
      "Validation boxes: 468\n",
      "Test: 797\n",
      "Test boxes: 1082\n"
     ]
    }
   ],
   "source": [
    "dataset_json = {\n",
    "    'Training': [],\n",
    "    'Validation': [],\n",
    "    'Test': []\n",
    "}\n",
    "\n",
    "for ds in datasets.keys():\n",
    "\n",
    "    # for training just add positive samples .. iterate over metadata\n",
    "    \n",
    "    if ds == 'Training':\n",
    "        scan_ids = datasets[ds]['metadata']['scan_id'].sort_values().unique().tolist()\n",
    "    else:\n",
    "        scan_ids = datasets[ds]['scans']['scan_id'].sort_values().tolist()\n",
    "\n",
    "    for scan_id in scan_ids:\n",
    "        annotation_dict = {\"box\" : [], \"image\" : f\"{scan_id.split('_')[0]}/{scan_id}.nii.gz\", \"label\" : []}\n",
    "\n",
    "        metadata = datasets[ds]['metadata'][datasets[ds]['metadata']['scan_id'] == scan_id]\n",
    "\n",
    "        for idx, row in metadata.iterrows():\n",
    "            annotation_dict['box'].append([\n",
    "                row['nodule_x_coordinate'],\n",
    "                row['nodule_y_coordinate'],\n",
    "                row['nodule_z_coordinate'],\n",
    "                row['nodule_diameter_mm'],\n",
    "                row['nodule_diameter_mm'],\n",
    "                row['nodule_diameter_mm']\n",
    "            ])\n",
    "\n",
    "            annotation_dict['label'].append(0)\n",
    "\n",
    "        dataset_json[ds].append(annotation_dict)\n",
    "\n",
    "# Check numbers in json\n",
    "print('Training:', len(dataset_json['Training']))\n",
    "print('Training boxes:', sum([len(x['box']) for x in dataset_json['Training']]))\n",
    "\n",
    "print('Validation:', len(dataset_json['Validation']))\n",
    "print('Validation boxes:', sum([len(x['box']) for x in dataset_json['Validation']]))\n",
    "      \n",
    "print('Test:', len(dataset_json['Test']))\n",
    "print('Test boxes:', sum([len(x['box']) for x in dataset_json['Test']]))\n",
    "\n",
    "# Save json\n",
    "import json\n",
    "with open(f'{workspace_path}/models/detection/datasplits/summit/partial/dataset_partial.json', 'w') as f:\n",
    "    json.dump(dataset_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that the files exist on the server before starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed Detection: 0\n"
     ]
    }
   ],
   "source": [
    "# Detection\n",
    "\n",
    "server_listings = open('detection_listings.txt', 'r').readlines()\n",
    "server_listings = [x.replace('./','').replace('\\n','') for x in server_listings]\n",
    "\n",
    "missed = 0\n",
    "for ds in datasets:\n",
    "    for image in dataset_json[ds]:\n",
    "        image_path = image['image']\n",
    "        if image_path not in server_listings:\n",
    "            print(f'Image {image_path} not found in server listings')\n",
    "            missed += 1\n",
    "            \n",
    "print('Missed Detection:', missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed GRT123: 0\n"
     ]
    }
   ],
   "source": [
    "# Grt123\n",
    "from yaml import scan\n",
    "\n",
    "\n",
    "server_listings = open('grt123_listings.txt', 'r').readlines()\n",
    "server_listings = [x.replace('_clean.npy','').replace('\\n','') for x in server_listings]\n",
    "\n",
    "missed = 0\n",
    "for ds in datasets:\n",
    "    \n",
    "    for scan_id in datasets[ds]['scans']['scan_id'].values:\n",
    "\n",
    "        \n",
    "        if scan_id not in server_listings:\n",
    "            print(f'Scan {scan_id} not found in server listings')\n",
    "            missed += 1\n",
    "\n",
    "print('Missed GRT123:', missed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
