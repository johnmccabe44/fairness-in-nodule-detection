{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double check sample numbers are correct for training SUMMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/john/Projects/SOTAEvaluationNoduleDetection\n",
      "Training scans: 4753\n",
      "Training metadata: 6145\n",
      "Training unique metadata scans: 2672\n",
      "Training excludes: 1428\n",
      "Validation scans: 297\n",
      "Validation metadata: 387\n",
      "Validation unique metadata scans: 173\n",
      "Validation excludes: 79\n",
      "Test scans: 891\n",
      "Test metadata: 1087\n",
      "Test unique metadata scans: 458\n",
      "Test excludes: 288\n",
      "Total scans in all datasets: 5941\n",
      "Total metadata in all datasets: 7619\n",
      "Unique scans in metadata: 3303\n",
      "Total excludes in all datasets: 1795\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "workspace_path = Path(os.getcwd()).parent.parent\n",
    "\n",
    "print(workspace_path)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'Training' : {},\n",
    "    'Validation': {},\n",
    "    'Test' : {}\n",
    "}\n",
    "\n",
    "for ds in datasets:\n",
    "\n",
    "    datasets[ds]['scans'] = pd.read_csv(f'{workspace_path}/metadata/summit/partial/{ds}_scans.csv')\n",
    "    datasets[ds]['metadata'] = pd.read_csv(f'{workspace_path}/metadata/summit/partial/{ds}_metadata.csv').assign(scan_id=lambda df: df.participant_id + '_Y0_BASELINE_A')\n",
    "    datasets[ds]['excludes'] = pd.read_csv(f'{workspace_path}/metadata/summit/partial/{ds}_excludes.csv').assign(scan_id=lambda df: df.participant_id + '_Y0_BASELINE_A')\n",
    "\n",
    "\n",
    "    print(f'{ds} scans: {datasets[ds][\"scans\"].shape[0]}')\n",
    "    print(f'{ds} metadata: {datasets[ds][\"metadata\"].shape[0]}')\n",
    "    print(f'{ds} unique metadata scans: {datasets[ds][\"metadata\"][\"scan_id\"].nunique()}')\n",
    "    print(f'{ds} excludes: {datasets[ds][\"excludes\"].shape[0]}')\n",
    "\n",
    "print('Total scans in all datasets:', sum([datasets[ds]['scans'].shape[0] for ds in datasets]))\n",
    "print('Total metadata in all datasets:', sum([datasets[ds]['metadata'].shape[0] for ds in datasets]))\n",
    "print('Unique scans in metadata:', sum([datasets[ds]['metadata']['scan_id'].nunique() for ds in datasets]))\n",
    "print('Total excludes in all datasets:', sum([datasets[ds]['excludes'].shape[0] for ds in datasets]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans fields: Index(['scan_id'], dtype='object')\n",
      "Metadata fields: Index(['form_instance_id', 'form_instance_status', 'participant_id',\n",
      "       'form_instance_index', 'nodule_brock_score', 'nodule_category',\n",
      "       'nodule_diameter_mm', 'nodule_lesion_id', 'nodule_lung_rads',\n",
      "       'nodule_mass', 'nodule_mass_core', 'nodule_mass_double_time_core',\n",
      "       'nodule_mass_doubling_time', 'nodule_reliable_segment', 'nodule_site',\n",
      "       'nodule_size_volume_cub_mm', 'nodule_slice_number',\n",
      "       'nodule_spiculation', 'nodule_subsolid_major_axis_diameter',\n",
      "       'nodule_type', 'nodule_volume_core', 'nodule_volume_doubling_time',\n",
      "       'nodule_volume_percentage_change',\n",
      "       'nodule_volume_volume_double_time_core', 'nodule_x_coordinate',\n",
      "       'nodule_y_coordinate', 'nodule_z_coordinate',\n",
      "       'radiology_report_management_plan_value', 'management_plan',\n",
      "       'radiology_report_malignancy_diagnosis',\n",
      "       'radiology_report_malignancy_criteria',\n",
      "       'radiology_report_malignancy_primary_order',\n",
      "       'radiology_report_scans_transfer_state', 'gender', 'age_group',\n",
      "       'ethnic_group', 'radiology_report_lesions_to_exclude', 'scan_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# What fields are in the datasets\n",
    "print('Scans fields:', datasets['Training']['scans'].columns)\n",
    "print('Metadata fields:', datasets['Training']['metadata'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset.json for MONAI Detection\n",
    "\n",
    "As the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 2845\n",
      "Training boxes: 6532\n",
      "Test: 891\n",
      "Test boxes: 1087\n"
     ]
    }
   ],
   "source": [
    "dataset_json = {\n",
    "    'Training': [],\n",
    "    'Test': []\n",
    "}\n",
    "\n",
    "for ds in datasets.keys():\n",
    "\n",
    "    # for training just add positive samples .. iterate over metadata\n",
    "    \n",
    "    if ds == 'Training' or ds == 'Validation':\n",
    "        scan_ids = datasets[ds]['metadata']['scan_id'].sort_values().unique().tolist()\n",
    "    \n",
    "    if ds == 'Test':\n",
    "        scan_ids = datasets[ds]['scans']['scan_id'].sort_values().tolist()\n",
    "\n",
    "    for scan_id in scan_ids:\n",
    "        annotation_dict = {\"box\" : [], \"image\" : f\"{scan_id.split('_')[0]}/{scan_id}.nii.gz\", \"label\" : []}\n",
    "\n",
    "        metadata = datasets[ds]['metadata'][datasets[ds]['metadata']['scan_id'] == scan_id]\n",
    "\n",
    "        for idx, row in metadata.iterrows():\n",
    "            annotation_dict['box'].append([\n",
    "                row['nodule_x_coordinate'],\n",
    "                row['nodule_y_coordinate'],\n",
    "                row['nodule_z_coordinate'],\n",
    "                row['nodule_diameter_mm'],\n",
    "                row['nodule_diameter_mm'],\n",
    "                row['nodule_diameter_mm']\n",
    "            ])\n",
    "\n",
    "            annotation_dict['label'].append(0)\n",
    "\n",
    "        dataset_json['Test' if ds == 'Test' else 'Training'].append(annotation_dict)\n",
    "\n",
    "# Check numbers in json\n",
    "print('Training:', len(dataset_json['Training']))\n",
    "print('Training boxes:', sum([len(x['box']) for x in dataset_json['Training']]))\n",
    "\n",
    "print('Test:', len(dataset_json['Test']))\n",
    "print('Test boxes:', sum([len(x['box']) for x in dataset_json['Test']]))\n",
    "\n",
    "# Save json\n",
    "import json\n",
    "with open(f'{workspace_path}/models/detection/datasplits/summit/partial/dataset_partial.json', 'w') as f:\n",
    "    json.dump(dataset_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that the files exist on the server before starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permissions</th>\n",
       "      <th>links</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>size</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>107275</td>\n",
       "      <td>Feb</td>\n",
       "      <td>8</td>\n",
       "      <td>16:14</td>\n",
       "      <td>listing.sh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>315490</td>\n",
       "      <td>Feb</td>\n",
       "      <td>9</td>\n",
       "      <td>14:59</td>\n",
       "      <td>listings.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>185564258</td>\n",
       "      <td>Sep</td>\n",
       "      <td>6</td>\n",
       "      <td>2023</td>\n",
       "      <td>summit-2222-djr_Y0_BASELINE_A.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>215327282</td>\n",
       "      <td>Sep</td>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>summit-2223-yts_Y0_BASELINE_A.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>175178964</td>\n",
       "      <td>Sep</td>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>summit-2224-eju_Y0_BASELINE_A.nii.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  permissions links    owner   group       size month day   time  \\\n",
       "1  -rw-r--r--     1  jmccabe  summit     107275   Feb   8  16:14   \n",
       "2  -rw-r--r--     1  jmccabe  summit     315490   Feb   9  14:59   \n",
       "3  -rw-r--r--     1  jmccabe  summit  185564258   Sep   6   2023   \n",
       "4  -rw-r--r--     1  jmccabe  summit  215327282   Sep   7   2023   \n",
       "5  -rw-r--r--     1  jmccabe  summit  175178964   Sep   7   2023   \n",
       "\n",
       "                               filename  \n",
       "1                            listing.sh  \n",
       "2                          listings.txt  \n",
       "3  summit-2222-djr_Y0_BASELINE_A.nii.gz  \n",
       "4  summit-2223-yts_Y0_BASELINE_A.nii.gz  \n",
       "5  summit-2224-eju_Y0_BASELINE_A.nii.gz  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed Detection: 0\n",
      "Training size: 472.614774511 GB\n"
     ]
    }
   ],
   "source": [
    "# Detection\n",
    "\n",
    "server_listings = open('detection_listings.txt', 'r').readlines()\n",
    "server_listings = [x.replace('/','').replace('\\n','') for x in server_listings]\n",
    "\n",
    "\n",
    "# Split each line into columns\n",
    "columns = [line.split() for line in server_listings]\n",
    "\n",
    "# Create a dataframe from the columns\n",
    "df = pd.DataFrame(columns).drop(0,axis=0)\n",
    "\n",
    "# Set column names\n",
    "df.columns = ['permissions', 'links', 'owner', 'group', 'size', 'month', 'day', 'time', 'filename']\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "missed = 0\n",
    "for ds in datasets:\n",
    "    for image in dataset_json['Test' if ds == 'Test' else 'Training']:\n",
    "        image_id = image['image'].split('/')[1]\n",
    "        if image_id not in df.filename.values:\n",
    "            print(f'Image {image_id} not found in server listings')\n",
    "            missed += 1\n",
    "        else:\n",
    "            # update listings df with dataset\n",
    "            df.loc[df.filename.str.contains(image_id), 'dataset'] = 'Test' if ds == 'Test' else 'Training'\n",
    "            \n",
    "print('Missed Detection:', missed)\n",
    "# Print the size of the training data\n",
    "bytes = df[df.dataset == 'Training']['size'].astype(int).sum()\n",
    "print('Training size:', bytes/1e9, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permissions</th>\n",
       "      <th>links</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>size</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drwxr-sr-x</td>\n",
       "      <td>2</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>307200</td>\n",
       "      <td>Mar</td>\n",
       "      <td>20</td>\n",
       "      <td>15:39</td>\n",
       "      <td>exclusions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>0</td>\n",
       "      <td>Apr</td>\n",
       "      <td>24</td>\n",
       "      <td>12:37</td>\n",
       "      <td>grt123_listings.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>22270688</td>\n",
       "      <td>Feb</td>\n",
       "      <td>9</td>\n",
       "      <td>16:01</td>\n",
       "      <td>summit-2223-yts_Y0_BASELINE_A_clean.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>160</td>\n",
       "      <td>Feb</td>\n",
       "      <td>9</td>\n",
       "      <td>16:01</td>\n",
       "      <td>summit-2223-yts_Y0_BASELINE_A_label.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-rw-r--r--</td>\n",
       "      <td>1</td>\n",
       "      <td>jmccabe</td>\n",
       "      <td>summit</td>\n",
       "      <td>16011992</td>\n",
       "      <td>Feb</td>\n",
       "      <td>8</td>\n",
       "      <td>23:04</td>\n",
       "      <td>summit-2224-gak_Y0_BASELINE_A_clean.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  permissions links    owner   group      size month day   time  \\\n",
       "1  drwxr-sr-x     2  jmccabe  summit    307200   Mar  20  15:39   \n",
       "2  -rw-r--r--     1  jmccabe  summit         0   Apr  24  12:37   \n",
       "3  -rw-r--r--     1  jmccabe  summit  22270688   Feb   9  16:01   \n",
       "4  -rw-r--r--     1  jmccabe  summit       160   Feb   9  16:01   \n",
       "5  -rw-r--r--     1  jmccabe  summit  16011992   Feb   8  23:04   \n",
       "\n",
       "                                  filename  \n",
       "1                               exclusions  \n",
       "2                      grt123_listings.txt  \n",
       "3  summit-2223-yts_Y0_BASELINE_A_clean.npy  \n",
       "4  summit-2223-yts_Y0_BASELINE_A_label.npy  \n",
       "5  summit-2224-gak_Y0_BASELINE_A_clean.npy  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed GRT123: 0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "# Grt123\n",
    "\n",
    "server_listings = open('grt123_listings.txt', 'r').readlines()\n",
    "server_listings = [x.replace('\\n','') for x in server_listings]\n",
    "\n",
    "\n",
    "# Split each line into columns\n",
    "columns = [line.split() for line in server_listings]\n",
    "\n",
    "# Create a dataframe from the columns\n",
    "df = pd.DataFrame(columns).drop(0,axis=0)\n",
    "\n",
    "# Set column names\n",
    "df.columns = ['permissions', 'links', 'owner', 'group', 'size', 'month', 'day', 'time', 'filename']\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "scans_on_server = df[df.filename.str.contains('_clean.npy')].filename.str.replace('_clean.npy','').values\n",
    "\n",
    "missed = 0\n",
    "for ds in datasets:\n",
    "    \n",
    "    for scan_id in datasets[ds]['scans']['scan_id'].values:\n",
    "\n",
    "        \n",
    "        if scan_id not in scans_on_server:\n",
    "            print(f'Scan {scan_id} not found in server listings')\n",
    "            missed += 1\n",
    "\n",
    "        else:\n",
    "            # update listings df with dataset\n",
    "            df.loc[df.filename.str.contains(scan_id), 'dataset'] = ds\n",
    "\n",
    "print('Missed GRT123:', missed)\n",
    "\n",
    "# Print the size of the training data\n",
    "bytes = df[df.dataset == 'Training']['size'].astype(int).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 86.670393324 GB\n"
     ]
    }
   ],
   "source": [
    "print('Training size:', bytes/1e9, 'GB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
