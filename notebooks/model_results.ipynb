{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import importlib\n",
    "import shutil\n",
    "\n",
    "sys.path.append('/Users/john/Projects/SOTAEvaluationNoduleDetection/utilities')\n",
    "sys.path.append('/Users/john/Projects/SOTAEvaluationNoduleDetection/models/grt123')\n",
    "sys.path.append('/Users/john/Projects/SOTAEvaluationNoduleDetection/models/grt123/training')\n",
    "sys.path.append('/Users/john/Projects/SOTAEvaluationNoduleDetection/models/grt123/preprocessing/')\n",
    "\n",
    "from layers import nms,iou\n",
    "from summit_utils import *\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LUNA16 Results Trained on LUNA16\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data = [\n",
    "    row.split('\\t')\n",
    "    for row in \"\"\"\n",
    "    Methods\t1/8\t1/4\t1/2\t1\t2\t4\t8\n",
    "    Liu et al. (2019)\t0.848\t0.876\t0.905\t0.933\t0.943\t0.957\t0.970\n",
    "    nnDetection (2021)\t0.812\t0.885\t0.927\t0.950\t0.969\t0.979\t0.985\n",
    "    MONAI detection\t0.835\t0.885\t0.931\t0.957\t0.974\t0.983\t0.988\n",
    "    \"\"\".split('\\n')\n",
    "    if row\n",
    "]\n",
    "data = pd.DataFrame(data).transpose().drop(0).drop(4, axis=1).rename(columns={0:'FPR',1:'grt123',2:'nnDetection',3:'monai'})\n",
    "data[['grt123','nnDetection','monai']] = data[['grt123','nnDetection','monai']].astype(float)\n",
    "\n",
    "def display_froc(fpr, recall, CADSystemName):\n",
    "\n",
    "    fig1 = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.plot(recall, fpr)\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('False positive rate')\n",
    "    plt.plot()\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel('Average number of false positives per scan')\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('FROC performance - %s' % (CADSystemName))\n",
    "    plt.grid(visible=True, which='both')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "display_froc(data.grt123, data.FPR, 'grt123')\n",
    "\n",
    "display_froc(data.grt123, data.FPR, 'monai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scans = pd.read_csv('../output/metadata/all/validation_scans.csv')\n",
    "validation_metadata = pd.read_csv('../output/metadata/all/validation_metadata.csv')\n",
    "print(f'There are {validation_metadata.shape[0]} nodules over {validation_scans.shape[0]} scans.')\n",
    "validation_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from evaluation import noduleCADEvaluation\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "MIN_THRESHOLD = -10000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRT123 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "RESULTS_PATH = '/Users/john/Projects/SOTAEvaluationNoduleDetection/output/results'\n",
    "METADATA_PATH = '/Users/john/Projects/SOTAEvaluationNoduleDetection/output/metadata'\n",
    "BBOX_RESULTS_PATH = '/Users/john/Projects/SOTAEvaluationNoduleDetection/models/grt123/bbox_result'\n",
    "MODEL = 'GRT123'\n",
    "NAME = 'all'\n",
    "PHASE = 'validation'\n",
    "\n",
    "combined_predictions = pd.read_csv(Path(BBOX_RESULTS_PATH, NAME, 'predictions.csv'))\n",
    "combined_nodule_data = pd.read_csv(Path(BBOX_RESULTS_PATH, NAME, 'metadata.csv'))\n",
    "scans_list = pd.read_csv(Path(METADATA_PATH, NAME, PHASE + '_scans.csv'))['scan_id'].tolist()\n",
    "\n",
    "print('Prediction:{}, Nodules:{}, Scans:{}'.format(\n",
    "    combined_predictions.shape[0],\n",
    "    combined_nodule_data.shape[0],\n",
    "    len(scans_list)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "    The evaluation script accepts 4 parameters\n",
    "\n",
    "    1. annotations_filename: csv file with following headers:\n",
    "        - threshold\n",
    "        - index\n",
    "        - row\n",
    "        - col\n",
    "        - diameter\n",
    "        - nodule_type\n",
    "        - brock_score\n",
    "        - management_plan\n",
    "        - ethnic_group\n",
    "\n",
    "    NOTE: Additional nodule fields will need to be added in eval script, fields are hard-coded\n",
    "    into a nodule class\n",
    "\n",
    "    2. annotations_excluded_filename: csv containing non-actionable or benign lesions,\n",
    "    headers as per annotations_filename\n",
    "\n",
    "    3. seriesuids_filename: single column csv file holding just the scan identifiers\n",
    "\n",
    "    4. results_filename: csv containing the candidates generated from the detection algorithm.\n",
    "    Headers as follows:\n",
    "        - threshold\n",
    "        - index\n",
    "        - row\n",
    "        - col\n",
    "        - diameter\n",
    "\n",
    "    5. outputdir: folder to output results to\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# double check output is as expected and numbers match up\n",
    "display(combined_predictions.head(), combined_predictions.shape)\n",
    "display(combined_nodule_data.head(), combined_nodule_data.shape)\n",
    "\n",
    "exclude  = combined_nodule_data.management_plan=='RANDOMISATION_AT_YEAR_1'\n",
    "include_nodule_data = combined_nodule_data[~exclude]\n",
    "exclude_nodule_data = combined_nodule_data[exclude]\n",
    "\n",
    "\n",
    "output_dir = Path(RESULTS_PATH, MODEL, NAME)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_dir = Path(output_dir, 'results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "annotations_filepath            = Path(output_dir, 'nodule_annotations.csv')\n",
    "annotations_exclude_filepath    = Path(output_dir, 'nodule_exclude_annotations.csv')\n",
    "predictions_filepath            = Path(output_dir, 'predictions.csv')\n",
    "scanlist_filepath               = Path(output_dir, 'scanslist.csv')\n",
    "\n",
    "\n",
    "include_nodule_data.to_csv(annotations_filepath, index=False)\n",
    "\n",
    "exclude_nodule_data.to_csv(annotations_exclude_filepath, index=False)\n",
    "\n",
    "combined_predictions.to_csv(predictions_filepath, index=False)\n",
    "\n",
    "with open(scanlist_filepath, 'w') as f:\n",
    "    for scan_id in scans_list:\n",
    "        f.write(scan_id)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noduleCADEvaluation(annotations_filename=annotations_filepath,\n",
    "                    annotations_excluded_filename=annotations_exclude_filepath,\n",
    "                    seriesuids_filename=scanlist_filepath,\n",
    "                    results_filename=predictions_filepath,\n",
    "                    outputDir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the misses\n",
    "\n",
    "\n",
    "def miss_check(model_name):\n",
    "    misses = (\n",
    "                pd.read_csv(f'../output/results/{model_name}/nodulesWithoutCandidate_{model_name}_predictions.txt', header=None)\n",
    "                .rename(columns={\n",
    "                    0:'name',\n",
    "                    1:'idx',\n",
    "                    2:'col',\n",
    "                    3:'row',\n",
    "                    4:'index',\n",
    "                    5:'diameter',\n",
    "                    6:'candidate_idx'}\n",
    "                )\n",
    "    )\n",
    "\n",
    "    misses = (\n",
    "                misses\n",
    "                .merge(combined_nodule_data, left_on='idx', right_on='id', suffixes=['_miss', ''])\n",
    "                .merge(validation_metadata, left_on='idx', right_index=True, suffixes=['', '_md'])\n",
    "    )\n",
    "\n",
    "\n",
    "    # Check distribution of type vs base distribution to see if one\n",
    "    # type of nodule is being missed\n",
    "    print('Nodule Type')\n",
    "    print((pd.concat([\n",
    "        misses.nodule_type.value_counts(normalize=True).rename('misses (%)'),\n",
    "        validation_metadata.nodule_type.value_counts(normalize=True).rename('all (%)')\n",
    "    ], axis=1).fillna(0) * 100).astype(int))\n",
    "\n",
    "    print(2 * '\\n')\n",
    "    print('Management Plan')\n",
    "    print((pd.concat([\n",
    "        misses.management_plan.value_counts(normalize=True).rename('misses (%)'),\n",
    "        validation_metadata.management_plan.value_counts(normalize=True).rename('all (%)')\n",
    "    ], axis=1).fillna(0) * 100).astype(int))\n",
    "\n",
    "    print(2 * '\\n')\n",
    "    print('Nodule Site')\n",
    "    print((pd.concat([\n",
    "        misses.nodule_site.value_counts(normalize=True, dropna=False).rename('misses (%)'),\n",
    "        validation_metadata.nodule_site.value_counts(normalize=True, dropna=False).rename('all (%)')\n",
    "    ], axis=1).fillna(0) * 100).astype(int))\n",
    "\n",
    "\n",
    "    print(2 * '\\n')\n",
    "    print('Nodule Diameter')\n",
    "    print((pd.concat([\n",
    "        misses.diameter.describe().rename('misses (%)'),\n",
    "        validation_metadata.nodule_diameter_mm.describe().rename('all (%)')\n",
    "    ], axis=1)))\n",
    "\n",
    "\n",
    "    print(2 * '\\n')\n",
    "    print('All - Site by Type (%)')\n",
    "    display(\n",
    "    (    pd.crosstab(\n",
    "            validation_metadata.nodule_type,\n",
    "            validation_metadata.nodule_site,\n",
    "            margins=True,\n",
    "            normalize=True\n",
    "        ) * 100).astype(int)\n",
    "    )\n",
    "\n",
    "    print(2 * '\\n')\n",
    "    print('Misses - Site by Type (%)')\n",
    "    display(\n",
    "    (    pd.crosstab(\n",
    "            misses.nodule_type,\n",
    "            misses.nodule_site,\n",
    "            margins=True,\n",
    "            normalize=True\n",
    "        ) * 100).astype(int)\n",
    "    )\n",
    "\n",
    "def get_missed_image_details(model_name):\n",
    "\n",
    "    misses = (\n",
    "            pd.read_csv(f'../output/results/{model_name}/nodulesWithoutCandidate_{model_name}_predictions.txt', header=None)\n",
    "            .rename(columns={\n",
    "                0:'name',\n",
    "                1:'idx',\n",
    "                2:'col',\n",
    "                3:'row',\n",
    "                4:'index',\n",
    "                5:'diameter',\n",
    "                6:'candidate_idx'}\n",
    "            ))\n",
    "\n",
    "    misses = misses.merge(validation_metadata.rename(columns={'index':'nodule_index'}), left_on='idx', right_index=True)\n",
    "\n",
    "    return [\n",
    "        (\n",
    "            missed_nodule['name'],\n",
    "            Ircd(\n",
    "                index=int(missed_nodule['index']),\n",
    "                row=int(missed_nodule['row']),\n",
    "                col=int(missed_nodule['col']),\n",
    "                diameter=int(missed_nodule['diameter'])\n",
    "            ),\n",
    "            missed_nodule['nodule_type'])\n",
    "        for idx, missed_nodule in misses.iterrows()\n",
    "    ]\n",
    "\n",
    "miss_check('grt123')\n",
    "\n",
    "grt123_missed_image_details = get_missed_image_details('grt123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The evaluation script accepts 4 parameters\n",
    "\n",
    "    1. annotations_filename: csv file with following headers:\n",
    "        - threshold\n",
    "        - index\n",
    "        - row\n",
    "        - col\n",
    "        - diameter\n",
    "        - nodule_type\n",
    "        - brock_score\n",
    "        - management_plan\n",
    "\n",
    "    NOTE: Additional nodule fields will need to be added in eval script, fields are hard-coded\n",
    "    into a nodule class\n",
    "\n",
    "    2. annotations_excluded_filename: csv containing non-actionable or benign lesions,\n",
    "    headers as per annotations_filename\n",
    "\n",
    "    3. seriesuids_filename: single column csv file holding just the scan identifiers\n",
    "\n",
    "    4. results_filename: csv containing the candidates generated from the detection algorithm.\n",
    "    Headers as follows:\n",
    "        - threshold\n",
    "        - index\n",
    "        - row\n",
    "        - col\n",
    "        - diameter\n",
    "\n",
    "    5. outputdir: folder to output results to\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "detect_nodule_annotations = (\n",
    "    validation_metadata\n",
    "    .rename(\n",
    "    columns={\n",
    "        'index' : 'idx',\n",
    "        'nodule_x_coordinate' : 'col',\n",
    "        'nodule_y_coordinate' : 'row',\n",
    "        'nodule_z_coordinate' : 'index',\n",
    "        'nodule_diameter_mm' : 'diameter'\n",
    "    })\n",
    "    .assign(name=lambda df: df.main_participant_id + '_Y0_BASELINE_A')\n",
    "    .assign(threshold=MIN_THRESHOLD)\n",
    ")[['name','threshold','index','row','col','diameter','nodule_type','nodule_brock_score','management_plan']]\n",
    "detect_nodule_annotations.index.rename('id',inplace=True)\n",
    "\n",
    "exclude = detect_nodule_annotations.management_plan=='RANDOMISATION_AT_YEAR_1'\n",
    "\n",
    "detect_nodule_exclude_annotations = detect_nodule_annotations[exclude]\n",
    "detect_nodule_exclude_annotations.to_csv('../output/results/detect_nodule_exclude_annotations.csv')\n",
    "\n",
    "detect_nodule_include_annotations = detect_nodule_annotations[~exclude]\n",
    "detect_nodule_include_annotations.to_csv('../output/results/detect_nodule_annotations.csv')\n",
    "\n",
    "scan_list = pd.read_csv('../output/metadata/validation_scans.csv')['scan_id'].tolist()\n",
    "with open('../output/results/detect_scanslist.csv', 'w') as f:\n",
    "    for scan_id in scans_list:\n",
    "        f.write(scan_id)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deconstruct the json output into the correct format of\n",
    "\n",
    "    threshold\n",
    "    index -> z\n",
    "    row -> y\n",
    "    col -> x\n",
    "    diameter\n",
    "    name -> scan_id\n",
    "\"\"\"\n",
    "import json\n",
    "with open('../models/summit_detection/result/result_summit_fold0.json','r') as f:\n",
    "    fold0_json = json.load(f)\n",
    "\n",
    "predictions = {}\n",
    "idx = 0\n",
    "for image in fold0_json['validation']:\n",
    "    name = image['image'].split('/')[-1].split('.',1)[0]\n",
    "    \n",
    "    for box, score in zip(image['box'], image['score']):\n",
    "        prediction = {}\n",
    "        prediction['threshold'] = score\n",
    "        prediction['index'] = box[2]\n",
    "        prediction['row'] = box[1]\n",
    "        prediction['col'] = box[0]\n",
    "        prediction['diameter'] = np.max(box[3:])\n",
    "        prediction['name'] = name\n",
    "\n",
    "        predictions[idx] = prediction\n",
    "        idx+=1\n",
    "\n",
    "detect_predictions = pd.DataFrame.from_dict(predictions,orient='index')\n",
    "detect_predictions.to_csv('../output/results/detect_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to subset\n",
    "scans_subset = detect_predictions.name.unique()\n",
    "detect_nodule_include_annotations = detect_nodule_include_annotations[detect_nodule_include_annotations.name.isin(scans_subset)]\n",
    "detect_nodule_include_annotations.to_csv('../output/results/detect_nodule_annotations.csv')\n",
    "\n",
    "detect_nodule_exclude_annotations = detect_nodule_exclude_annotations[detect_nodule_exclude_annotations.name.isin(scans_subset)]\n",
    "detect_nodule_exclude_annotations.to_csv('../output/results/detect_nodule_exclude_annotations.csv')\n",
    "\n",
    "with open('../output/results/detect_scanslist.csv', 'w') as f:\n",
    "    for scan_id in scans_subset:\n",
    "        f.write(scan_id)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noduleCADEvaluation(annotations_filename='../output/results/detect_nodule_annotations.csv',\n",
    "                    annotations_excluded_filename='../output/results/detect_nodule_exclude_annotations.csv',\n",
    "                    seriesuids_filename='../output/results/detect_scanslist.csv',\n",
    "                    results_filename='../output/results/detect_predictions.csv',\n",
    "                    outputDir='../output/results/detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_check('detect')\n",
    "\n",
    "detect_missed_image_details = get_missed_image_details('detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether there are any nodules that both missed\n",
    "from math import isclose\n",
    "\n",
    "def same_nodule(miss_1, miss_2):\n",
    "\n",
    "    if miss_1[0] == miss_2[0] and isclose(miss_1[1].diameter, miss_2[1].diameter, abs_tol=0.1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "both_missed = [\n",
    "    grt_miss\n",
    "    for grt_miss in grt123_missed_image_details\n",
    "    for dct_miss in detect_missed_image_details\n",
    "    if same_nodule(grt_miss, dct_miss)\n",
    "]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grt123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
